\section{Verifiable randomness}
\label{sec:vrf}

\subsection{Introduction to VRFs}
Generating random numbers is useful in many applications. Random numbers can be generated using pseudo-random number generators, which take a seed as input and produces a series of uniformly random and unpredictable bit strings. A limitation of PRNGs is that they are not verifiable. One cannot verify that the random number generated is actually random without knowing the seed. If, however, one knows the seed, the random number is no longer unpredictable to the verifier. In a setting where unpredictable random numbers are needed, but we cannot assume a trusted party to generate them, we have to be able to generate a random number which can be verified to have been unpredictable at the time of computation.

\cite{micali_verifiable_1999} define a VRF as 3 algorithms, a function generator $G$, a function evaluator ($F_1$, $F_2$), and a function verifier $V$. G receives a unary string as input and generates an asymmetric key pair (PK, SK). $F_1$ receives the SK and an input x, and generates a random value, while $F_2$ generates a proof from SK and x. V receives PK, x, v, and proof and outputs either yes or no.

For the purpose of our subject, a VRF needs to be unpredictable at some time of commitment, and computable and verifiable at some time of revealing. Below, we will outline some schemes that are used to achieve this in a adversarial network setting.


\subsection{Categories of VRFs}

\subsubsection{N of N  secret commitments}
A two-step protocol where N parties generate a random number that is only known after all parties reveal a secret.
In the first round, participants commit to some secret bits which they hash and publish. In the second and last round, participants reveal their secret bits, i.e. the preimage of their hash. All the secret bits are aggregated (e.g. by XOR or concatenation), the result of the aggregation being the random seed. We see that the random seed will be unpredictable unless one knows the secrets of all the participants. Assuming the secrets are chosen randomly, we will get a uniformly random bitstring as seed.
This scheme is quite simple, but has some weaknesses. It can fail if one participant does not reveal their secret. The last participant to reveal their secret can see the outcome before they reveal their own secret, and so choose to not reveal if the outcome is unfavorable. Participants can however be incentivized to cooperate. This scheme can face scalability issues, as the risk of one participant going offline increases with the number of participants.

\subsubsection{M of N secrets}
A threshold multisignature scheme can be used to generate an unpredictable seed that will be revealed after a subset of participants sign a message.
A multisignature scheme works so that the ability to sign a message is spread among $N$ private keys. $M$ is a threshold parameter which means the number of private keys are needed to sign the message in order to provide a valid signature. Each private key signing a message provides a signature share and a validity proof. A combining function takes $M$ validity proofs and signature shares and outputs a valid signature and a proof or it fails. By using the valid signature from the combining function as a random seed, we can make a verifiable random number that needs the cooperation of at least $M$ members of the committee of $N$ members.  
We see that the random number will be unpredictable as long as there is no collusion of more than $M-1$ members. If we assume there is no such collusion, this scheme has several advantages from the above scheme. It will be able to provide a random number even if $N-M$ participants are unresponsive. When $M-1$ participants have shared their signature share, any of the remaining signers can privately see what the result is by making the last share without sharing it. They could then avoid sharing their share if the random number is unfavorable, but any one of them could continue the process by publishing their share. This is different from the above scheme in that any one of the participants won't be able to halt the scheme alone â€“ unless $N=M$, of course.

\subsubsection{Kademlia DHT aggregation}
Discussed in \cite{grumbach_distributed_2017}. Participants are assigned an address in a Kademlia DHT. Nodes cooperate with their neighbors to aggregate secrets they have committed to. Each sub-tree will have an aggregation of the secrets from its participants which are leaf nodes. The aggregation of the root node will be the random seed. 
This is an interactive protocol that can scale quite well, as a Kademlia DHT is proven to be scalable. It does, however, assume an honest majority of 50\%. 

\subsection{Delay functions}
A delay function is an algorithm that is expected to take a predefined minimum amount of time to calculate. If the participants can agree on an input to the delay function whose output is the random seed, it is guaranteed that nobody will be able to know the random seed before the minimum time of the delay function has passed. The ideal delay function is inherently sequential, low variance, moderately hard to compute, and easy to verify. A parallelizable delay function can be calculated arbitrary fast by an adversary with many processors. A delay function with high variance will sometimes be calculated very quickly, and sometimes very slowly. A delay function that is too easy or too hard does not achieve its purpose. A delay function that is easy to verify saves verifiers from repeating the work of provers, and does not discourage verification.
While the properties of a delay function can be very appealing, as its input can be an aggregation of a large amount of contributors, the assumptions of a delay function can be restrictive. One must assume that adversaries and verifiers have somewhat equal computing power. As application specific integrated circuits (ASICs) can calculate specific algorithms order of magnitudes faster than commodity CPUs, one must assume that there is no ASICs or that enough honest participants have ASICs, so that good parameters can be set and verification is feasible. 

\subsubsection{Example of using a delay function}
Let's say we need a verifiable random number that should be unpredictable before some point in time $t_{end}$. In order to make sure no one is able to predict the random number before $t_{end}$, we say the random number is the result of a delay function. By making assumptions of current hardware and algorithms, we estimate the delay function has a minimum calculation time of $td_{min}$. The delay function takes some input that we must agree upon, e.g. a blockchain block hash which is available at time $t_{start}$. As long as the input to the delay function is unknown by $t_{start}$ and the assumption of minimum time needed to calculate the delay function holds, we can be certain that the random number is unpredictable by $t_{end}$. Note that we assume that the delay function is a random oracle under the random oracle model, and that the input is not known before the start time. In this example we use the a block hash. Even though some bits of a block hash can be manipulated by an adversary, the block hash of blockchain with a large hashing power cannot be manipulated so strongly that one can calculate the delay function of a hash of the adversary's choosing. We could also use other inputs such as an aggregation of commitments from multiple participants.

\subsubsection{Hash chain}
A hash chain can be used as a delay function is inherently sequential, has low variance, arbitrarily hard to compute, hard to verify. A hash chain is a sequence of hashes that are linked by each item being the hash of the previous item. A hash chain of length $n$ and input $h_0$ is the composition of a hash function $H$ $n$ times which we call $H_n$. What makes a hash chain sequential is that one cannot calculate $H_n$ without knowing $H_{n-1}$, and it cannot be calculated in parallel as one step's output is the next step's input. The time needed to calculate a hash chain is not probabilistic, so the variance of calculation time will be negligible. It can be made arbitrarily hard as the time needed to calculate a hash chain will increase linearly with its length. A drawback is that a verifier would need to calculate the entire hash chain as well in order to validate its correctness. 

\subsubsection{Proof of work (PoW)}
PoW is not sequential, has high variance, is arbitrarily hard to compute, easy to verify. Since PoW is parallelizable and has high variance, it's not really good as a delay function. An adversary can get lucky and calculate the result quickly, and thus avoid the delay. Calculating a PoW can be seen as a probabilistic process where one makes attempts that result in success or failure. The probability of success for each trial is equal and independent of any other trial. If multiple parties tries to calculate the same PoW, but starting from different nonces, the distribution of time elapsed for each party will have high variance.  

\subsubsection{Weakly encrypted bits}
Participants can weakly encrypt a secret by e.g. using a short encryption key, so that the secret is verifiable either after some time or when the participant reveals their secret.
This is discussed in \cite{syverson_weakly_1998}, but is not completely satisfactory. Secrets can be encrypted so that brute-forcing the encryption key is inherently sequential, but one cannot prove that breaking it is possible. (AFAICU, read Goldschlag et al. 2010)
It's also an issue that breaking many weakly encrypted secrets is parallelizable, so it's hard to choose a delay parameter which makes it both feasible to break the encryption and protect against adversaries with much computing power.


\subsection{Random beacon}
A random beacon is a public source of randomness. The beacon can be considered a trusted third party in a transaction between untrusting parties, so that they all have a random number they can agree on. \cite{rabin1983transaction} discuss the use of random beacons for transaction verification, but does not outline the design of a verifiable random beacon. Using sources of randomness that are hard to forge, such as financial data \cite{clark_use_2010} and blockchain block hashes \cite{bentov_bitcoin_2016} \cite{yajam_improvement_2019} has been suggested, but is not considered safe from manipulation \cite{pierrot_malleability_2018}. NIST and random.org maintain public random beacons that are accessible over a web API. Although these organizations claim that their randomness is truly random and comes from sources of high entropy, relying on their servers for randomness ultimately involves trust in their organization. 
