\section{Verifiable randomness}
\label{sec:vrf}

\subsection{Introduction to verifiable random functions (VRF)}

Generating random numbers is useful in many applications. Random numbers can be generated using pseudo-random number generators (PRNG), which take a seed as input and produce a series of uniformly random and unpredictable bit strings. A limitation of PRNGs is that they are not verifiable. If one is presented a random number generated from a PRNG, one cannot verify that the number is actually random without knowing the seed, as it could be an arbitrary number. If, however, one knows the seed, the random number is no longer unpredictable to the verifier. In a setting where unpredictable random numbers are needed, but we cannot assume a trusted party to generate them, we have to be able to generate a random number which can be verified to have been unpredictable at the time of computation.

Micali et al. in \cite{micali_verifiable_1999} define a VRF as 3 algorithms: a function generator $G$, a function evaluator ($F_1$, $F_2$), and a function verifier $V$. G receives a unary string as input and generates an asymmetric key pair $(PK, SK)$. $F_1$ receives the $SK$ and an input $x$, and generates a random value, while $F_2$ generates a proof from $SK$ and $x$. $V$ receives $PK, x, v, proof$ and outputs either $yes$ or $no$.

A set of participants that do not trust each other needs to generate a random number that is unpredictable by all. The random number cannot come from an outside source, so the participants need to follow some protocol to generate it. It must be computable, so it cannot remain unpredictable forever, as it's obviously not unpredictable after it has been computed. If a protocol allows the participants can agree on an input to a random function without being able to compute it until a later point in time, we can use that as a VRF.

With the exception of the explanation of random beacons, the rest of this section will introduce some known implementations of such a protocol.

\subsection{Categories of VRFs}

\subsubsection{N of N  secret commitments}
A two-step protocol where N parties generate a random number that is only known after all parties reveal a secret.
In the first round, participants generate a secret bitstring, calculate its hash, and publicly commit to the hash. In the second and last round, participants reveal their secret bits, i.e. the preimage of the hash they committed to. All the secret bits are aggregated, e.g. by XOR or concatenation, the result of the aggregation being the random seed. We see that the random seed will be unpredictable unless one knows the secrets of all the participants, and infeasible to guess if the secret bitstrings are of a minimum length. Assuming the secrets are chosen randomly, we will get a uniformly random bitstring as seed.
This scheme is quite simple, but has some weaknesses. It can halt if one participant does not reveal their secret. The last participant to reveal their secret can see the outcome before they reveal their own secret, and so choose to not reveal if the outcome is unfavorable. Participants can however be incentivized to cooperate, e.g. by losing reputation if failing to reveal their secret. This scheme can face scalability issues, as the risk of one participant going offline increases with the number of participants.

\subsubsection{M of N secrets}
A threshold multisignature scheme can be used to generate an unpredictable seed that will be revealed after a subset of participants sign a message.
A multisignature scheme works so that the ability to sign a message is spread among $N$ private keys. $M$ is a threshold parameter which represent the number of private keys that are needed to sign the message in order to provide a valid signature. Each private key signing a message provides a signature share and a validity proof. A combining function takes $M$ validity proofs and signature shares and outputs a valid signature and a proof or it fails. By using the valid signature from the combining function as a random seed, we can make a verifiable random number that needs the cooperation of at least $M$ members of a committee of $N$ members.  
We see that the random number will be unpredictable as long as there is no collusion of more than $M-1$ members. If we assume there is no such collusion, this scheme has several advantages from the above scheme. It will be able to provide a random number even if $N-M$ participants are unresponsive. When $M-1$ participants have shared their signature share, any of the remaining signers can privately see what the result is by generating the last share without sharing it. They could then avoid sharing their share if the random number is unfavorable, but any one of them could continue the process by publishing their share. This is different from the above scheme in that any one of the participants won't be able to halt the scheme alone – unless $N=M$, of course.

\subsubsection{Kademlia aggregation}
Discussed by Grumbach and Riemann in \cite{grumbach_distributed_2017}. Participants are assigned an address in a Kademlia distributed hash table (DHT) \cite{maymounkov2002kademlia}. Nodes cooperate with their neighbors to aggregate secrets they have committed to. Each sub-tree in the DHT topology can make an aggregate of values from its children. These values will at the leaf nodes be secrets from participants in the DHT. The aggregation of the root node will be the random seed used in this VRF scheme. 
This is an interactive protocol that can scale quite well, as a Kademlia DHT is proven to be scalable. It does, however, assume an honest majority of 50\%. 

\subsection{Delay functions}
A delay function is an algorithm that is expected to take a predefined minimum amount of time to calculate. If the participants can agree on an input to the delay function whose output will be used as a random seed, it is guaranteed that nobody will be able to know the random seed before the minimum time of the delay function has passed. Some delay functions are probabilistic in that they only have an expected minimum time to compute. We say the variance of a delay function is the variance of the distribution of time elapsed when calculating the delay function. An ideal delay function is inherently sequential, has low variance, is moderately hard to compute, and easy to verify.

A parallelizable delay function can be calculated arbitrary fast by an adversary with many processors. A delay function with high variance will sometimes be calculated very quickly, and sometimes very slowly. A delay function that is too easy or too hard does not achieve its purpose. A delay function that is easy to verify saves verifiers from repeating the work of provers, and does not discourage verification.

While the properties of a delay function can be very appealing, as its input can be an aggregation of a large amount of contributors, the assumptions of a delay function can be restrictive. One must assume that adversaries and verifiers have somewhat equal computing power. As application specific integrated circuits (ASIC) can calculate specific algorithms order of magnitudes faster than commodity CPUs, one must assume that there are no ASICs or that enough honest participants have ASICs. If honest participants and potential adversaries have approximately equal computing power, good parameters for the delay function can be set and verification will be feasible.

\subsubsection{Example of using a delay function}
Let's say we need a verifiable random number that should be unpredictable before some point in time $t_{end}$. In order to make sure no one is able to predict the random number before $t_{end}$, we say the random number is the result of a delay function. By making assumptions of current hardware and algorithms, we estimate the delay function has a minimum calculation time of $td_{min}$. The delay function takes some input that we must agree on, e.g. a blockchain block hash which is available at time $t_{start}$. As long as the input to the delay function is unknown by $t_{start}$ and the assumption of minimum time needed to calculate the delay function holds, we can be certain that the random number is unpredictable by $t_{end}$. Note that we assume that the delay function is a random oracle under the random oracle model, and that the input is not known before the start time. In this example we use the a block hash. Even though some bits of a block hash can be manipulated by an adversary, the block hash of blockchain with a large hashing power cannot be manipulated so strongly that one can calculate the delay function of a hash of the adversary's choosing. We could also use other inputs such as an aggregation of commitments from multiple participants.

\subsubsection{Hash chain}
A hash chain of a certain length used as a delay function is inherently sequential, has low variance, can be arbitrarily hard to compute, and is hard to verify. A hash chain is a sequence of hashes that are linked by each item being the hash of the previous item. A hash chain of length $n$ and input $h_0$ is the composition of a hash function $H$ $n$ times which we call $H_n$. What makes a hash chain sequential is that one cannot calculate $H_n$ without knowing $H_{n-1}$, and it cannot be calculated in parallel as one step's output is the next step's input. The time needed to calculate a hash chain is not probabilistic, so the variance of calculation time will be negligible. It can be made arbitrarily hard as the time needed to calculate a hash chain will increase linearly with its length. A drawback is that a verifier would need to calculate the entire hash chain in order to validate its correctness. 

\subsubsection{Proof of work (PoW)}
PoW is not sequential, has high variance, is arbitrarily hard to compute, and is easy to verify. Since PoW is parallelizable and has high variance, it's not really good as a delay function. An adversary can get lucky and calculate the result quickly, and thus avoid the delay. An adversary with multiple processors can calculate a PoW multiple times faster than an honest participant with one processor. Calculating a PoW can be seen as a probabilistic process where one makes attempts that result in success or failure. The probability of success for each trial is equal and independent of any other trial. If multiple parties tries to calculate the same PoW, but starting from different nonces, the distribution of time elapsed for each party will have high variance.  

\subsubsection{Weakly encrypted bits}
Participants can weakly encrypt a secret by e.g. using a short encryption key, so that the secret is verifiable either after the time needed to brute force the decryption, or when the participant reveals their encryption key.
This is discussed in \cite{syverson_weakly_1998}, but is not completely satisfactory as a delay function, as breaking a secret encrypted with a short key is parallelizable. 
A scheme to combine a delay function with a weakly encrypted secret is suggested in \cite{rivest_time-lock_1996}. The scheme works as follows: A secret $s$ is encrypted with a strong key $K$. There is a function $w()$ that is easy to calculate if one knows a secret $n$ but is inherently sequential to invert, and can be initialized with a parameter that says how many computations are needed to invert it.
This can be used to generate unpredictable numbers by having participants commit to a commitment $(c_1, c_2)$ where $c_1=Encrypt(s, K)$ and $c_2=w(K)$. We assume nobody has been able to find all $K$ by brute forcing all $w^{-1}(c_2)$ within some time limit, when participants reveal their $K$ and $n$. It must be verified that all $c_2=w(K)$ before all $s$ are calculated and aggregated to form a random seed. If a participant fails in revealing $K$, it can be calculated by eventually solving $w{-1}(c_2)$.

Note that $Decrypt(c_1, k)$ will provide a value for all k, so we must verify that $c_2=w(K)$. $w^{-1}(c_2)$ must therefore be guaranteed to have a solution that is possible to calculate within reasonable time. Otherwise, a single anti-social participant could halt the process by committing to a $c_2$ that takes one year instead of, say, one hour to brute force. 


\subsection{Random beacons}
A random beacon is a service that broadcasts random numbers or provides random numbers on request, and can be used as a public source of randomness. The beacon can be considered a trusted third party in a transaction between untrusting parties, so that they all have a random number they can agree on. Rabin in \cite{rabin1983transaction} discuss the use of random beacons for transaction verification, but do not outline the design of a verifiable random beacon. Using sources of randomness that are hard to forge, such as financial data \cite{clark_use_2010} and blockchain block hashes \cite{bentov_bitcoin_2016, yajam_improvement_2019} has been suggested, but is not considered safe from manipulation \cite{bonneau2015bitcoin, pierrot_malleability_2018}. NIST \cite{nist_nist_2019} and random.org maintain public random beacons that are accessible over a web API, but their randomness is not verifiable and an application using it will have to trust these organizations. 

\subsection{Verifiable random oracles}

Using random oracles for randomness often involves trusting the random oracle to act honestly. However, a random oracle can also use a random process that is verifiable. RanDAO \cite{randao2015randao} is a random oracle that can be spawned as a smart contract. RanDAO uses an N-of-N commitment scheme, and will output the result of that process if all N players reveal their secret or an error otherwise. As discussed above, any one player in such a scheme is able to halt the process and force an error to be returned if they fail to reveal their secret. This makes it possible for an adversary to tactically not reveal secrets if the result is unfavorable. To prevent – or at least discourage – such behaviour, participants of RanDAO can be forced to make a deposit in order to join the process. The deposit will be confiscated if a player fails to follow the protocol. Such a deposit would have to be so high that the potential gains from halting the random process does not exceed the amount that is confiscated. An actor using a N-of-N commitment scheme with deposits that can be confiscated as a random oracle could then to some degree trust that the random process will not be manipulated if there is at least one honest player – i.e. non-colluding player – that submits a uniformly random secret.

The scheme above with deposits makes the assumptions that: there is at least one honest player, and one honest player submits a uniformly random number. The former assumption is important because it makes sure that nobody can predict the outcome. All the players could be colluding and know each other's secrets and thus predict the outcome. The latter is also important, as it could be the case that players submit a secret that is not uniformly random, and that an adversary can use this fact to predict the output. To demonstrate this, consider if it were common that all players simply submitted a bitstring of zeroes as their secret. The process would succeed and will be verifiable, but the outcome would be predictable. The RanDAO contract is spawned as a random oracle to be used by another application, and the players in the RanDAO don't necessarily care about producing a uniformly random number. Even if players are rewarded for participating in the RanDAO, they would get the reward whether they submit a uniformly random number or something else.

The issue of not being able to verify that secrets in RanDAO and similar schemes are actually random is addressed in \cite{chatterjee_probabilistic_2019}. A scheme similar to N-of-N secret commitments is used with the addition of a reward mechanism that favours players submitting uniformly random numbers. For players maximizing their reward, the authors prove that a quasi-strong equilibrium in a game-theoretic sense exist when players submit uniformly random numbers. The scheme otherwise functions as a RanDAO that will either return a random number if all players followed the protocol, or an error otherwise. Players also have to deposit an amount so high that the potential gains of halting the process is lower than the deposit. A reward to players is paid so that any player playing the optimal game of submitting uniformly random numbers gets an expected outcome of at least $\frac{reward}{n\_players}$. 

The above scheme allows one to spawn a random oracle that will produce a number that is uniformly random if players want to maximize their reward, and that is unpredictable as long as at least one player is honest. The scheme still suffers from a principal-agent problem, in that even if the random process, the agent, is executed correctly, it may not act in the best interest of the application that spawned it, the principal. An important assumption made is that players are non-colluding so that no single actor knows all the secrets in the scheme. This is encouraged by rewarding players and having the process being open for anyone who can make a deposit. The players will maximize their reward by playing honestly, and punished by not following the protocol, so what's the problem? 
There is a problem when the potential gains from being able to predict the outcome is higher than what it costs to bribe players to reveal their secrets. Players in such a scheme could potentially gain more by revealing their secret to the highest bidder than what can be gained by playing honestly and compete for the in-game reward. If selling the secret is not punishable, then there is not much to lose for a player of the process in the random oracle. The scheme suggested in \cite{chatterjee_probabilistic_2019} could be amended with a mechanism that confiscates the player's deposit if their secret is revealed by someone other than player themselves, but this issue is not discussed in the work.
